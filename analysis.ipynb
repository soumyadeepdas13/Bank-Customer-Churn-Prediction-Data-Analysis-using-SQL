{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2477c92",
   "metadata": {},
   "source": [
    "# Customer Churn Analysis Notebook\n",
    "\n",
    "Professional notebook with EDA, preprocessing, baseline model and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de40ed",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "\n",
    "Load dataset and show basic info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a92c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "repo = Path('.')\n",
    "df = pd.read_csv('data/Bank_Churn_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48f7df6",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "Include key polished images: churn, age distribution, correlation matrix, ROC and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a5ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image('images/churn_pie_polished.png'))\n",
    "display(Image('images/age_hist_polished.png'))\n",
    "display(Image('images/corr_matrix_polished.png'))\n",
    "display(Image('images/roc_curve_polished.png'))\n",
    "display(Image('images/confusion_matrix_polished.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132add4",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Feature Engineering\n",
    "\n",
    "Brief description and code to prepare features for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e420af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing sample\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df2 = df.copy()\n",
    "for col in ['RowNumber','CustomerId','Surname']:\n",
    "    if col in df2.columns:\n",
    "        df2 = df2.drop(columns=[col])\n",
    "if 'Geography' in df2.columns:\n",
    "    df2 = pd.get_dummies(df2, columns=['Geography'], drop_first=True)\n",
    "if 'Gender' in df2.columns:\n",
    "    df2['Gender'] = df2['Gender'].map({'Female':0,'Male':1}).fillna(0)\n",
    "\n",
    "y = df2['Exited']\n",
    "X = df2.drop(columns=['Exited'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "print('Prepared train/test sets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b734a",
   "metadata": {},
   "source": [
    "## 4. Baseline Model (Logistic Regression)\n",
    "\n",
    "Train a baseline logistic regression model and evaluate with AUC and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961aed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "proba = model.predict_proba(X_test)[:,1]\n",
    "pred = model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, pred))\n",
    "print('AUC:', roc_auc_score(y_test, proba))\n",
    "print('Confusion matrix:\\n', confusion_matrix(y_test, pred))\n",
    "print('\\nClassification report:\\n', classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e1d472",
   "metadata": {},
   "source": [
    "## 5. Save Model & Next Steps\n",
    "\n",
    "Model saved in `models/logistic_regression_baseline.pkl`. Next steps include hyperparameter tuning, feature selection, and more advanced models."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
